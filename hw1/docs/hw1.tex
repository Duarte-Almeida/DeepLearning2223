\documentclass{exam}

\usepackage{titling}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{float}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{inconsolata}
\usepackage{tikz}
\usepackage{physics}
\usepackage{amsthm}
\usepackage[font={small}]{caption}

\graphicspath{ {./pictures} }

\setlength{\droptitle}{-5em}   

%\renewcommand{\questionlabel}{\textbf{~\thequestion)}}
%\renewcommand{\subpartlabel}{\thesubpart)}
%\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}

\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

\newenvironment{shiftedflalign*}{%
    \start@align\tw@\st@rredtrue\m@ne
    \hskip\parindent
}{%
    \endalign
}

\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}

\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}

\newtheorem{lemma}{Lemma}

\title{%
  Homework 1\\
  \vspace{0.5em}
  \large Deep Learning }
\author{
  Duarte Calado de Almeida\\
  95565
  \and
  Andr√© Lopes Rodrigues\\
  96576
}
\date{}

\cfoot{\thepage}

\begin{document}
    \maketitle
    \begin{tikzpicture}[overlay, remember picture]
        \node[xshift=3.5cm,yshift=-2cm] at (current page.north west) {\includegraphics[scale = 0.35]{logo_ist.jpg}};
    \end{tikzpicture}

    \section*{Question 1}

    \section*{Question 2}

    \section*{Question 3}
    \begin{questions}
        \question
        Let $x_i$ denote the $i$-th component of $\vb*{x}$ and let $W_{ij}$ denote the entry in the $i$-th row and $j$-th column in matrix $\vb*{W}$. We then have that:

        \begin{align*}
            h_i(\vb*{x}) &= g\left(\sum_{j = 1}^{D} W_{ij}x_j \right) = \left(\sum_{j = 1}^{D} W_{ij}x_j \right)^2 = \left(\sum_{j = 1}^{D} W_{ij}x_j\right)\left( \sum_{k = 1}^{D} W_{ik}x_k\right) \\
            &= \sum_{j = 1}^{D} \sum_{k = 1}^{D} W_{ij}x_j  W_{ik}x_k = \sum_{j = 1}^{D} W_{ij}^2x_j^2 +  \sum_{j = 1}^{D} \sum_{k \neq j} W_{ij}W_{ik}x_j x_k \\
            &= \sum_{j = 1}^{D} W_{ij}^2x_j^2 + \sum_{j = 1}^{D} \sum_{k = 1}^{j - 1}  W_{ij}W_{ik}x_j x_k + 
            \sum_{j = 1}^{D} \sum_{k = j + 1}^{D}  W_{ij}W_{ik}x_j x_k \\
            &= \sum_{j = 1}^{D} W_{ij}^2x_j^2 +\sum_{k = 1}^{D} \sum_{j = k + 1}^{D}  W_{ij}W_{ik}x_j x_k + 
            \sum_{j = 1}^{D} \sum_{k = j + 1}^{D}  W_{ij}W_{ik}x_j x_k \\
            &= \sum_{j = 1}^{D} W_{ij}^2x_j^2 + \sum_{k = 1}^{D} \sum_{j = k + 1}^{D} W_{ij}W_{ik}(2x_j x_k) \\
            &= 
            \begin{bmatrix}
                W_{i1}^2 & W_{i1}W_{i2} & \dots & W_{i1}W_{iD} & W_{i2}^2 & W_{i2}W_{i3} & \dots & W_{i(D - 1)}^2  & W_{i(D - 1)} W_{iD} &  W_{iD}^2
            \end{bmatrix}
            \begin{bmatrix}
                x_1^2 \\ 2x_1 x_2 \\ \dots \\ 2x_1 x_D \\ x_2^2 \\ 2x_2 x_3 \\ \dots \\ x_{D - 1}^2 \\ 2x_{D - 1} x_{D} \\  x_{D}^2
            \end{bmatrix}
        \end{align*}
        As such, $\vb*{h}$ is linear in some feature transformation $\vb*{\phi}$, that is, $\vb*{h}$ can be written as $\vb*{A}_{\Theta}\vb*{\phi}(\vb*{x})$. In particular, we have that such matrix $\vb*{A}_{\Theta}$ can be defined as:
        \begin{equation*}
            \vb*{A}_{\Theta} = 
            \brows{\vb*{a}_1^T \\ \vb*{a}_2^T \\ \rowsvdots \\ \vb*{a}_D^T}
        \end{equation*}
        where
        \begin{equation*}
            \vb*{a}_i = 
            \begin{bmatrix}
            W_{i1}^2 & W_{i1}W_{i2} & \dots & W_{i1}W_{iD} & W_{i2}^2 & W_{i2}W_{i3} & \dots & W_{i(D - 1)}^2  & W_{i(D - 1)} W_{iD} &  W_{iD}^2
            \end{bmatrix}^T
        \end{equation*}
        and we can define the feature transformation $\vb*{\phi}$ as:
        \begin{equation*}
            \vb*{\phi}(\vb*{x}) = (x_1^2, 2x_1 x_2, \dots, 2x_1 x_D, x_2^2, 2x_2 x_3, \dots, x_{D - 1}^2, 2x_{D - 1} x_{D}, x_{D}^2)
        \end{equation*}

        \question
        Given that the predicted output $\hat{y}$ is defined as:
        \[
            \hat{y} = \vb*{v}^T \vb*{h}
        \]
        the linearity of $\vb*{h}$ in the feature transformation $\vb*{\phi}(\vb*{x})$ proven above leads to following equality:
        \[
            \hat{y} = \vb*{v}^T \vb*{A}_{\Theta} \vb*{\phi}(\vb*{x}) 
                    = (\vb*{A}_{\Theta}^T \vb*{v})^T \vb*{\phi}(\vb*{x}) 
                    = \vb*{c}_{\Theta}^T \vb*{\phi}(\vb*{x}) 
        \]
        where we take $\vb*{c}_{\Theta}$ to be equal to $\vb*{A}_{\Theta}^T \vb*{v}$, thereby proving that $\hat{y}$ is also a linear transformation of $\vb*{\phi}(\vb*{x})$. Nevertheless, $\hat{y}$ is \textbf{not} linear in terms of the original parameters $\Theta$. To see this, note that the model is now a linear combination of \textbf{products} of entries of $\vb*{W}$ and $\vb*{v}$ rather than the entries by themselves:
        \[
            \hat{y} = \vb*{v}^T \vb*{A}_{\Theta} \vb*{\phi}(\vb*{x}) = \sum_{i = 1}^{D} v_i (w_i^T \vb*{x})^2 
            = \sum_{i = 1}^{K} \sum_{j = 1}^{D} \sum_{k = 1}^{D} v_i W_{ij} W_{ik} x_i x_k
        \]
        where we define $w_i$ to be the $i$-th row vector of matrix $\vb*{W}$.

        \question
        \begin{proof}[\unskip\nopunct]
        To prove the desired result, for $\vb*{c}_{\Theta}$ defined in the previous subquestion and for any $\vb*{c} \in \mathbb{R}^{\frac{D(D + 1)}{2}}$, we make the observation that the inner products $\vb*{c}_{\Theta}^T  \vb*{\phi}(\vb*{x})$ and $\vb*{c}^T  \vb*{\phi}(\vb*{x})$ actually correspond to quadratic forms in $\vb*{x}$:
        \begin{align*}
            \vb*{c}_{\Theta}^T \vb*{\phi(x)} &= \sum_{i = 1}^K v_i (\vb*{A}_{\Theta} \vb*{\phi}(\vb*{x}))^2_i = 
            \sum_{i = 1}^K v_i (w_i^T \vb*{x})^2_i \\
            &= 
            \begin{bmatrix}
                w_1^T \vb*{x} & w_2^T \vb*{x} & \dots & w_K^T \vb*{x}
            \end{bmatrix}
            \text{diag}(\vb*{v})
            \begin{bmatrix}
                w_1^T \vb*{x} \\ w_2^T \vb*{x} \\ \dots \\ w_K^T \vb*{x}
            \end{bmatrix} \\
            &= (\vb*{Wx})^T \text{diag}(\vb*{v}) \vb*{Wx} 
             = \vb*{x}^T \vb*{W}^T \text{diag}(\vb*{v}) \vb*{Wx} 
        \end{align*}
        and 
        \begin{align*}
            \vb*{c}^T \vb*{\phi(x)} &= \sum_{i = 1}^{\frac{D(D + 1)}{2}} c_i \phi_i(x) = \sum_{i = 1}^{D} c_{(i - 1)D + i}\; x_i^2 + \sum_{i = 1}^{D}\sum_{j = i + 1}^{D} c_{(i - 1)D + j}\;(2 x_i x_j) \\
            &= \vb*{x}^T \mathcal{M}(\vb*{c}) \vb*{x}
        \end{align*}
        where $\mathcal{M}(\vb*{c}) \in \mathbb{R}^{D \times D}$ is a symmetric matrix obatined through $\vb*{c}$ such that:
        \begin{itemize}
            \item[--] the diagonal and the part above the diagonal of the matrix $\mathcal{M}(\vb*{c})$ is filled row-wise with the elements of vector $\vb*{c}$, i.e.:
            \[
                \mathcal{M}(\vb*{c}) = 
                \begin{bmatrix}
                    c_1 & c_2 & c_3 & \dots & c_D \\
                    c_2 & c_{D + 1} & c_{D + 2} & \dots & c_{2D - 1} \\
                    \vdots & \vdots & \vdots & \ddots & \vdots \\
                    c_D & c_{D - 1} & c_{D - 2} & \dots & c_{\frac{D(D + 1)}{2}} 
                \end{bmatrix}
            \]
            \item[--] for $1 \le 1 \le D$ and $j < i$, $(\mathcal{M}(\vb*{c}))_{ij} = (\mathcal{M}(\vb*{c}))_{ji}$
        \end{itemize}
        Furthermore, we also recurr to the following lemma:

        \begin{lemma}
            Two vectors $\vb*{a}, \: \vb*{b} \in \mathbb{R}^{\frac{D(D + 1)}{2}}$ are equal if and only if $\vb*{a}^T \vb*{\phi(x)} = \vb*{b}^T \vb*{\phi(x)}$, for all $\vb*{x} \in \mathbb{R}^D$
        \end{lemma}
        \begin{proof}
            If $\vb*{a} = \vb*{b}$, then $\vb*{a}^T \vb*{\phi(x)} = \vb*{b}^T \vb*{\phi(x)}$ is trivially verified. For the reverse implication, note that, according to the previously made observation, for any $\vb*{x} \in \mathbb{R}^D$:
            \[
                \vb*{a}^T \vb*{\phi(x)} = \vb*{b}^T \vb*{\phi(x)} \Rightarrow \vb*{x}^T \mathcal{M}(\vb*{a}) \vb*{x} = \vb*{x}^T \mathcal{M}(\vb*{b}) \vb*{x}
            \]
            Since both $\mathcal{M}(\vb*{a})$ and $\mathcal{M}(\vb*{a})$ are symmetric and the associated quadratic forms are twice differentiable continuous, taking the hessian on both sides of the equation yields:
            \[
                \mathcal{M}(\vb*{a}) = \mathcal{M}(\vb*{b})
            \]
            that is, $\mathcal{M}(\vb*{a})$ and $\mathcal{M}(\vb*{b})$ are entrywise equal. In particular, we have that $a_i = b_i$, for $i = 1, \dots, \frac{D(D + 1)}{2}$.
        \end{proof}
        We are now equipped with the tools needed for the proof. Since $\mathcal{M}(\vb*{c})$ is symmetric, the \textbf{Spectral Decomposition Theorem} tells us that there is a matrix $\vb*{Q}$ orthonormal and $\vb*{\Lambda}$ diagonal such that $\mathcal{M}(\vb*{c}) = \vb*{Q} \vb*{\Lambda}\vb*{Q}^T$. Let $\vb*{q_i}$ denote the eigenvector of $\mathcal{M}(\vb*{c})$ that is present in $i$-th column of $\vb*{Q}$ and let $\lambda_i$ be the corresponding eigenvector (note that $\{\vb*{q}_i\}_{i = 1}^{D}$ forms an orthonormal basis of $\mathbb{R}^{D}$). Then, we can write $\vb*{c}^T \vb*{\phi(x)}$ as:
        \[
            \vb*{c}^T \vb*{\phi(x)} =  \vb*{x}^T \mathcal{M}(\vb*{c}) \vb*{x} = (\vb*{Q}^T \vb*{x})^T \vb*{\Lambda} (\vb*{Q}^T \vb*{x}) = \sum_{i = 1}^{D} \lambda_i (\vb*{q}_i^T\vb*{x})^2
        \]
        Now, if we assume that $K \ge D$, then we can construct the matrix $\vb*{W}$ and vector $\vb*{v}$ that make $\vb*{c}^T_{\Theta} \vb*{\phi(x)}$ equal to $\vb*{c}^T \vb*{\phi(x)}$ in the following way:
        \begin{itemize}
            \item[--] we make $\vb*{v}$ to be equal to $(\lambda_1, \lambda_2, \dots, \lambda_D, \underbrace{0, \dots, 0}_{K - D \: \text{times}})$;
            \item[--] we make $\vb*{W}$ to be equal to the vertical concatenation of $\vb*{Q}^T$ with a $(K - D) \times D$ matrix of zeros, i.e.:
            \begin{align*}
            \vb*{W} = 
                \begin{bmatrix}
                    &\vb*{Q}^T \\
                    &\vb*{0}_{(K - D) \times D}
                \end{bmatrix}
            \end{align*}
        \end{itemize}
        We then have that:
        \[
            \vb*{c}^T_{\Theta} \vb*{\phi(x)} = \sum_{i = 1}^{K} \lambda_i (\vb*{w}_i^T\vb*{x})^2 =  \sum_{i = 1}^{D} \lambda_i (\vb*{w}_i^T\vb*{x})^2 = (\vb*{Q}^T \vb*{x})^T \vb*{\Lambda} (\vb*{Q}^T \vb*{x}) = \vb*{x}^T \vb*{Q} \vb*{\Lambda}\vb*{Q}^T \vb*{x} = \vb*{c}^T \vb*{\phi(x)}
        \]
        and, by Lemma 1, we prova that the previous choice of $\vb*{W}$ and $\vb*{v}$ originate a vector $\vb*{c}_{\Theta}$ such that $\vb*{c}_{\Theta} = \vb*{c}$
        \end{proof}
        In fact, we can relax the requirement $K \ge D$ to be $K \ge D - N$, where $N$ is the dimension of the nullspace of $\mathcal{M}(\vb*{c})$. Since $\mathcal{M}(\vb*{c})$ is symmetric and thus diagonalizable, the set of indices $\mathcal{I}$ such that the eigenvectors $\{\vb*{q}_i | i \in \mathcal{I}\}$ are not associated with eigenvalue zero has $D - N$ elements. As such:
        \[
            \vb*{c}^T \vb*{\phi(x)} = \sum_{i = 1}^{D} \lambda_i (\vb*{q}_i^T\vb*{x})^2 = \sum_{i \in \mathcal{I}} \lambda_i (\vb*{q}_i^T\vb*{x})^2
        \]
        and thus we can take $\vb*{W}$ to be equal to the concatenation of the matrix $\vb*{\tilde{Q}}^T$ with a matrix of $N \times K$ zeros (where $\vb*{\tilde{Q}} \in \mathbb{R}^{D \times (D - N)}$ and $\vb*{\tilde{q}}_k = \vb*{q}_{i_k}$, $1 \le k \le D - N$). Following the previously made argument, we would obtain again a vector $\vb*{c}_{\Theta}$ equal to $\vb*{c}$. \\
        Now, if $K < D - N$, then the nullspace of $\vb*{W}$ has at least dimension $D - (D - N - 1) = N + 1$. Since the rank of $\mathcal{M}(\vb*{c})$ is $D - N$, the dimensions of the row space of $\mathcal{M}(\vb*{c})$ and the nullspace of $\vb*{W}$ sum up to $D + 1$ and thus there is exists some non-null vector $\vb*{x^*}$ that is in the rowspace of $\mathcal{M}(\vb*{c})$ and in the nullspace of $\vb*{W}$. Let $\vb*{y^*} = \vb*{Q}^T \vb*{x^*}$ and choose $\vb*{c}$ to be a vector such that $\mathcal{M}(\vb*{c})$ is positive semidefinite, namely: 
        \[
            \mathcal{M}(\vb*{c}) = \text{diag}(\underbrace{1, \dots, 1}_{D - N \text{times}}, \underbrace{0, \dots, 0}_{N \text{times}}) 
        \]
        we have:
        \[
            \vb*{c}^T_{\Theta} \vb*{\phi(x^*)}
            = \vb*{x^*}^T \vb*{W}^T \text{diag}(\vb*{v}) \vb*{Wx^*} = \vb*{0}
        \]
        and
        \[
            \vb*{c}^T \vb*{\phi(x^*)} = \sum_{i \in \mathcal{I}} \lambda_i (\vb*{q}_i^T\vb*{x^*})^2 > 0
        \]
        since $\lambda_i > 0$ ($i \in \mathcal{I}$), and $(\vb*{q}_i^T\vb*{x^*})^2 > 0$ for at least one $i$, as $x^*$ belongs to the rowspace of $ \mathcal{M}(\vb*{c})$, and, consequently, it does not belong to its nullspace.\\
        We have thus constructed an instance where $K < D$ and there is no choice of parameters $\vb*{W}$ and $\vb*{v}$ that make $\vb*{c}^T_{\Theta}$ equal to $\vb*{c}$.
        \question
    \end{questions}
\end{document}